{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIFmEHSrZJppZ0z2BbdIhM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Training the model with Triplet Hard Loss"],"metadata":{"id":"1ztgZWy8d6Mw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_XscIO6Mvd7"},"outputs":[],"source":["# Inputs for Anchor, positive and negative images\n","anchor_in = Input(name='anchor',shape=target_shape)\n","pos_in = Input(name='positive',shape=target_shape)\n","neg_in = Input(name='negative',shape=target_shape)\n","\n","# Extract embeddings using VGG19\n","anchor_out = embedding(anchor_in)\n","pos_out = embedding(pos_in)\n","neg_out = embedding(neg_in)\n","\n","# Define the model\n","model_triplet_loss = Model(inputs=[anchor_in, pos_in, neg_in], outputs=[anchor_out,pos_out,neg_out])\n","\n","model_triplet_loss.summary()"]},{"cell_type":"code","source":["class SiameseModel(Model):\n","\n","  \"\"\"The Siamese Network model with a custom training and testing loops.\n","\n","    Computes the triplet loss using the three embeddings produced by the\n","    Siamese Network.\n","\n","    The triplet loss is defined as:\n","       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n","    \"\"\"\n","    \n","\n","  def __init__(self, siamese_network, margin=0.5):\n","      super().__init__()\n","      self.siamese_network = siamese_network\n","      self.margin = margin\n","      self.loss_tracker = metrics.Mean(name='loss')\n","        \n","  def call(self, inputs):\n","      return self.siamese_network(inputs)\n","    \n","  def train_step(self, data):\n","    # GradientTape is a context manager that records every operation that\n","    # you do inside. We are using it here to compute the loss so we can get\n","    # the gradients and apply them using the optimizer specified in\n","    # `compile()`.\n","\n","    with tf.GradientTape() as tape:\n","\n","      loss = self._compute_loss(data)\n","\n","      # Storing the gradients of the loss function with respect to the\n","      # weights/parameters.\n","\n","      gradients = tape.gradient(loss,self.siamese_network.trainable_weights)\n","\n","      # Applying the gradients on the model using the specified optimizer\n","      self.optimizer.apply_gradients(\n","                    zip(gradients,self.siamese_network.trainable_weights)\n","        )\n","        \n","      # Let's update and return the training loss metric\n","      self.loss_tracker.update_state(loss)\n","      return {\"loss\": self.loss_tracker.result()}\n","    \n","  def test_step(self, data):\n","    loss = self._compute_loss(data)\n","    # Let's update and return the loss metric.\n","    self.loss_tracker.update_state(loss)\n","    return {\"loss\": self.loss_tracker.result()}\n","    \n","  def _compute_loss(self, data):\n","    # Computing Triplet Hard loss\n","    anchor, positive, negative = self.siamese_network(data)\n","      \n","    # Computing the Triplet Loss by subtracting both distances and\n","    # making sure we don't get a negative value.\n","    pos_dist = tf.reduce_sum(tf.square(anchor-positive), -1)\n","    neg_dist = tf.reduce_sum(tf.square(anchor-negative), -1)\n","\n","    # Compute the hardest positive and negative examples for each anchor.\n","    hardest_positives = K.max(pos_dist, axis=0, keepdims=True)\n","    hardest_negatives = K.min(neg_dist, axis=0, keepdims=True)\n","\n","    loss = hardest_positives - hardest_negatives + self.margin\n","    loss = tf.maximum(loss, 0.0)\n","    \n","    return K.mean(loss)\n","    \n","  @property\n","  def metrics(self):\n","    return [self.loss_tracker]"],"metadata":{"id":"KQt0AMpvd8v6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["siamese_model = SiameseModel(model_triplet_loss)\n","siamese_model.compile(optimizer=Adam(learning_rate=0.0001))"],"metadata":{"id":"f3R_m5kCd-Aa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history_hard_triplet = siamese_model.fit(train_data, validation_data=val_data, steps_per_epoch=len(train_data)//8,validation_steps = len(val_data)//8,\n","epochs=10) "],"metadata":{"id":"y-JCJq_-d_A2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train loss vs validation loss\n","plt.plot(history_hard_triplet.history['loss'],label='Train Loss')\n","plt.plot(history_hard_triplet.history['val_loss'],label='Validation Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"QXiSm-f0d_3O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Embedding layer\n","\n","triplet_hard_embeddings = model_triplet_loss.layers[-1]\n","triplet_hard_embeddings"],"metadata":{"id":"M4wdpzdZeBAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["triplet_hard_embeddings.save(\"triplet_hard_embeddings.h5\")\n","triplet_hard_embeddings.save_weights(\"triplet_hard_embeddings.h5\")"],"metadata":{"id":"KNCI8mlPeCLh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What is the problem with triplet loss?\n","\n","So what is the problem, it seems to work fine, doesn’t it?\n","\n","- The issue is with this line of the loss function.\n","```\n","loss = K.maximum(basic_loss,0.0)\n","```\n","- There is a major issue here, every time your loss gets below 0, you lose information, a ton of information. First let’s look at this function.\n","\n","\n","- It basically does this:\n","\n","<img src=\"https://drive.google.com/uc?id=1ej1BjkTxwFf_SUke0GxhlJyGOUbnlekC\">\n","\n","- It tries to bring close the Anchor (current image) with the Positive (A image that is similar with the Anchor) as far as possible from the Negative (A image that is different from the Anchor)\n","\n","The actual formula for this loss is:\n","$L(A,P,N)=max(0,d(A,P)-d(A,N)+\\alpha)$\n","\n","\n","Let’s pretend that:\n","- Alpha is 0.2\n","- Negative Distance is 2.4\n","- Positive Distance is 1.2\n","\n"," - The loss function result will be 1.2–2.4+0.2 = -1. \n","\n"," - Then when we look at Max(-1,0) we end up with 0 as a loss.\n","\n"," - The Positive Distance could be anywhere above 1 and the loss would be the same.\n","\n"," - With this reality, it’s going to be very hard for the algorithm to reduce the distance between the Anchor and the Positive value.\n","\n","As a more visual example, here is 2 scenarios A and B. They both represent what the loss function measure for us.\n","\n","<img src=\"https://drive.google.com/uc?id=1dtzGxkuZE0lBBd_gj_JlWKyuNdUMj1GH\">\n","\n","> $A=1.2-2.4-0.4=-1.6$\n","\n","> $B=0.2-2.4-0.4=-2.6$\n","\n","- After the Max function both A and B now return 0 as their loss, which is a clear lost of information.\n","- By looking simply, we can say that B is better than A.\n","\n","> To make a loss function that will capture the “lost” information below 0.\n","-If you contain the N dimension space where the loss is calculated you can more efficiently control this.\n","- So the first step was to modify the model.\n","- The last layer (Embedding layer) needed to be controlled in size.\n","-By using a Sigmoid activation function instead of a linear we can guarantee that each dimension will be between 0 and 1.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"xnfvW03neFi-"}},{"cell_type":"markdown","source":["So Lets change the activation function to sigmoid in the last layer of the embeddings"],"metadata":{"id":"VNTmLSFYeKMO"}},{"cell_type":"markdown","source":["#### **Quiz-4**\n","\n","How to make the triplet loss capture the lost information below 0?\n","\n","(a) Increasing the number of neurons in the last layer of the embedding.\n","\n","(b) Controlling the size of last layer of embedding by using sigmoid activation function instead of linear.\n","\n","(c) Using tanh activation function in the last layer of the embedding\n","\n","**Answer:** (b)"],"metadata":{"id":"tpsDLmcUeLhn"}},{"cell_type":"code","source":["input_dim = target_shape =  (180,180,3)"],"metadata":{"id":"8-umy-yeeDQp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_cnn = resnet.ResNet50(\n","    weights=\"imagenet\", input_shape=input_dim, include_top=False\n",")\n","\n","base_cnn.trainable=False\n","\n","glob_pool = layers.GlobalAveragePooling2D()(base_cnn.output)\n","dense1 = layers.Dense(128)(glob_pool)\n","output = layers.Dense(128,activation='sigmoid')(dense1)\n","\n","embedding_new = Model(base_cnn.input, output, name=\"Embedding\")\n","\n","# trainable = False\n","# for layer in base_cnn.layers:\n","#     if layer.name == \"conv5_block1_out\":\n","#         trainable = True\n","#     layer.trainable = trainable"],"metadata":{"id":"MWvCKytteM2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inputs for Anchor, positive and negative images\n","anchor_in = Input(name='anchor',shape=target_shape)\n","pos_in = Input(name='positive',shape=target_shape)\n","neg_in = Input(name='negative',shape=target_shape)\n","\n","# Extract embeddings using VGG19\n","anchor_out = embedding_new(anchor_in)\n","pos_out = embedding_new(pos_in)\n","neg_out = embedding_new(neg_in)\n","# Concatenate the embeddings\n","# merged_vector = concatenate([anchor_out, pos_out, neg_out],axis=1,name='triplet_layer')\n","\n","# loss = layers.Lambda(triplet_loss)([anchor_out,pos_out,neg_out])\n","# distances = DistanceLayer()(\n","#         anchor_out,\n","#         pos_out,\n","#         neg_out\n","#     )\n","\n","\n","# Define the model\n","model_lossless_triplet_loss = Model(inputs=[anchor_in, pos_in, neg_in], outputs=[anchor_out,pos_out,neg_out])\n","\n","# Compiling the model using Adam optimizer and triplet loss\n","# model_triplet_loss.compile(optimizer=Adam(learning_rate=1e-3, epsilon=1e-01))\n","model_lossless_triplet_loss.summary()"],"metadata":{"id":"TwNvfReXeNzx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we have changed the activation function of the last layer of the embedding to **sigmoid activation**\n","\n","But, the problem with triplet loss is\n","- loss becomes zero when trained for more epochs\n","\n","### How can we solve this?\n","\n","- We can create a loss function which breaks the linearity in cost function.\n","- In other words, make it really costly as more the error grows.\n","\n","Here comes the **lossless triplet loss**\n","\n","It is defined as \n","\n","$\\sum_{i=1} ^{n}[-ln(-\\frac{(f^a_{i}-f^p_{i})^2}{\\beta})+1+\\epsilon)-ln(-\\frac{N-(f^a_{i}-f^n_{i})^2}{\\beta})+1+\\epsilon)]$\n","\n","- Where N is the number of dimensions (Number of output of your network; Number of features for your embedding)\n","- β is a scaling factor.\n","- $\\epsilon$ is the margin\n","- $(f^a_{i}-f^p_{i})^2$ is the distance between anchor and positive\n","- $(f^a_{i}-f^n_{i})^2$ is the distance between anchor and negative\n","\n"],"metadata":{"id":"mCX8BfpUeQx-"}},{"cell_type":"code","source":["# Custom Function for lossless triplet loss\n","def lossless_triplet_loss(y_true, y_pred, beta=3, epsilon=1e-8):\n","    anchor, positive, negative =y_pred[0,0:512], y_pred[0,512:1024], y_pred[0,1024:1536]\n","    \n","    pos_dist = K.mean(K.square(anchor - positive),axis=-1)\n","    neg_dist = K.mean(K.square(anchor - negative),axis=-1)\n","    \n","    N=3\n","    pos_dist = -tf.math.log(-tf.divide((pos_dist),beta)+1+epsilon)\n","    neg_dist = -tf.math.log(-tf.divide((N-neg_dist),beta)+1+epsilon)\n","    loss = neg_dist + pos_dist\n","    \n","    return loss"],"metadata":{"id":"5TX48q8TeOzZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SiameseModel(Model):\n","    \n","    def __init__(self, siamese_network, margin=0.5):\n","        super().__init__()\n","        self.siamese_network = siamese_network\n","        self.margin = margin\n","        self.loss_tracker = metrics.Mean(name='loss')\n","        self.N = 3\n","        self.beta = 3\n","        self.epsilon = 1e-8\n","        \n","    def call(self, inputs):\n","        return self.siamese_network(inputs)\n","    \n","    def train_step(self, data):\n","        with tf.GradientTape() as tape:\n","            loss = self._compute_loss(data)\n","        gradients = tape.gradient(loss,self.siamese_network.trainable_weights)\n","        self.optimizer.apply_gradients(\n","                    zip(gradients,self.siamese_network.trainable_weights)\n","        )\n","        \n","        self.loss_tracker.update_state(loss)\n","        return {\"loss\": self.loss_tracker.result()}\n","    \n","    def test_step(self, data):\n","        loss = self._compute_loss(data)\n","        self.loss_tracker.update_state(loss)\n","        return {\"loss\": self.loss_tracker.result()}\n","    \n","    def _compute_loss(self, data):\n","        anchor, positive, negative = self.siamese_network(data)\n","        pos_dist = K.mean(K.square(anchor - positive),axis=-1)\n","        neg_dist = K.mean(K.square(anchor - negative),axis=-1)\n","        pos_dist = -tf.math.log(-tf.divide((pos_dist),self.beta)+1+self.epsilon)\n","        neg_dist = -tf.math.log(-tf.divide((self.N-neg_dist),self.beta)+1+self.epsilon)\n","        loss = neg_dist + pos_dist\n","    \n","        return loss \n","    \n","    @property\n","    def metrics(self):\n","        return [self.loss_tracker]"],"metadata":{"id":"HlutCXWFeSW6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["siamese_model_lossless = SiameseModel(model_lossless_triplet_loss)\n","siamese_model_lossless.compile(optimizer=Adam(learning_rate=0.00001))"],"metadata":{"id":"DU3xZIuYeTgK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history_lossless = siamese_model_lossless.fit(train_data, validation_data=val_data, steps_per_epoch=len(train_data)//8,validation_steps = len(val_data)//8,\n","epochs=10) "],"metadata":{"id":"xUOzkFEFeVfB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_lossless_triplet_loss.layers"],"metadata":{"id":"th1D8VvDeWZC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["signature_embeddings_new = model_lossless_triplet_loss.layers[-1]"],"metadata":{"id":"SyLHRzaueX55"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["signature_embeddings_new.save(\"signature_embeddings_new.h5\")\n","signature_embeddings_new.save_weights(\"signature_embeddings_new_weights.h5\")"],"metadata":{"id":"8o-Zz2A4eY65"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_distances_new = []"],"metadata":{"id":"R7zf49oteaCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(inp_imgs)):\n","  inp_feat = preprocess_image(inp_imgs[i])\n","  val_feat = preprocess_image(val_imgs[i])\n","  res = L2(signature_embeddings_new.predict(np.expand_dims(inp_feat,axis=0)),signature_embeddings_new.predict(np.expand_dims(val_feat,axis=0)))\n","  pred_distances_new.append(res)"],"metadata":{"id":"t5ij0wm0ebSR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = pairs[2].values"],"metadata":{"id":"Ail548yTectY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_distances_new_ = []"],"metadata":{"id":"mSLXgjaceeAi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(pred_distances_new)):\n","  pred_distances_new_.append(pred_distances_new[i][0][0])"],"metadata":{"id":"KMEm5w3Vee8B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc,thresh = compute_accuracy_thresh(pred_distances_new_,labels)"],"metadata":{"id":"m4UGNB-Wef3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Accuracy \n","acc"],"metadata":{"id":"k-lDEDynehG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Threshold\n","thresh"],"metadata":{"id":"iYXk7wQpeiNy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Conclusion**\n","\n","- As you can see above, when the model is trained with triplet loss and lossless triplet loss, the embeddings learn to differentiate between the similar images and dis-similar images clearly.\n","\n","- Lossless triplet loss is better than the standard triplet loss because:\n"," - In **Standard triplet loss**, if loss value becomes negative then, it'll be zero, which is a clear lost of information\n"," - In **lossless triplet loss**, the non-linearity is introduced which makes it really costly as more the error grows.\n","\n","- So, we can say that **Lossless Triplet Loss** is the best loss function for pairwise learning which can be used in siamese network for best performance.\n"],"metadata":{"id":"Hcdy_yzoejj4"}}]}